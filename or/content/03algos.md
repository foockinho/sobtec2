# Le code est politique, les algorithmes sont des armes mathématiques de destruction [^1]

***Benjamin Cadon***

![](../../contrib/gfx/illustrations/algoritmos-halfsize.png)

On en entend beaucoup parler, on ne les voit jamais, que sont ces algorithmes, ces bêtes invisibles et insaisissables qui se faufilent dans nos cerveaux et habitent nos poches? Quels desseins les animent?

D’un point de vu formel, un algorithme n’est qu’une inoffensive suite d’opérations alimentée par des données qui produit un résultat. Néanmoins, ils automatisent la résolution d’un ensemble de problèmes complexes[^2] et certains sont ainsi devenus des Intelligences Artificielles élevées, grâce à des entreprises qui les gavent des données qu’on a bien voulu leur donner gratuitement.

## Un bestiaire[^3] d’algorithmes

Rien de tel que de savoir de quoi ils se nourrissent pour mieux les cerner et comprendre leur rôle dans la société des humains informatisés. Ils ne sont pas nés d’une étincelle d’électricité au dessus d’un océan de sulfureuses données. Leurs géniteurs sont des êtres humains qui écrivent des lignes de code pour réaliser un programme porteur d’un projet politique et sociétal dicté par un commanditaire privé ou public. Ces algorithmes ne sont jamais « neutres » et impartiaux et vont s’attacher à mener la mission qui leur a été assignée, souvent par des Occidentaux de genre masculin issus des classes supérieures bercées par le capitalisme.

Il faut aussi dire qu’un algorithme bête nourri avec beaucoup de bonnes données réussira toujours mieux qu’une intelligence artificielle affamée, et ce, même si elle a les griffes acérées.

Comment ne pas citer ces ogres américains que sont les GAFAM (pour Google, Apple, Facebook, Amazon et Microsoft)ou BATX pour leurs alter-égos de l’autre coté du Pacifique (les géants du Web chinois Baidu, Alibaba, Tencent et Xiaomi). Leur métabolisme est basé sur le fait de collecter, avec notre aide, un maximum de données sur nos moindres faits et gestes en « augmentant » notre quotidien avec pléthore d’applications mobiles et d’objets connectés supposés nous le rendre plus facile à vivre.

### Qui mangent des données personnelles 

Les algorithmes qui en résulte sont polymorphes. Ils ont tout d’abord grandi en nous regardant de loin, scrutant notre activité sur le réseau, les endroits que nous fréquentions le plus. Ils se sont ensuite élevés au-dessus de nos interactions afin de mieux déterminer qui faisait autorité, passant de la logique du vote populaire à celle du classement méritocratique. Dans un troisième mouvement, ils sont rentrés dans notre intimité numérique en analysant la qualité et la fréquence de nos échanges afin d'évaluer notre réputation et de traquer nos affinités. Enfin, ils se sont cachés au-dessous de notre regard pour mieux prédire le moindre de nos désirs, tout en œuvrant à les conditionner.

| _                     | **A côté**                | **Au-dessus**                | **Dans**                   | **Au-dessous**             |
|-----------------------|---------------------------|------------------------------|----------------------------|----------------------------|
| **Exemples**          | Médiamétrie, Google Analytics, affichage publicitaire | PageRank de Google, Digg, Wikipédia | Nombre d’amis Facebook, Retweet de Twitter, notes et avis | Recommandations Amazon, publicités comportementales |
| **Données**           | Vues | Liens | Likes | Traces |
| **Population**        | Échantillon représentatif | Vote censitaire, communautés | Réseau social, affinitaire, déclaratif | Comportements individuels implicites |
| **Forme de calcul**   | Vote | Classement méritocratique | Benchmark | Machine Learning |
| **Principe**          | Popularité | Autorité | Réputation | Prédiction |

*D’après Dominique Cardon, « À quoi rêvent les algorithmes »* [^4]

Ces différentes générations d’algorithmes continuent à cohabiter et sont facilement reconnaissables car ils se montrent très efficaces pour nous rendre de nombreux services, si tenté que l’on paye notre « dividende numérique » [^5], parce qu'ils discrétisent notre existence, la coupant en tranches aussi fines que possible pour en extraire toute information [^6] monétisable.

Chaque État materne un ogre terrifiant qui œuvre dans le renseignement. Celui ci croise souvent ses intérêts avec ceux de ses amis ogres commerçants, en allant piocher sans vergogne dans leurs gardes-manger, et ce avec leur assentiment [^7]. Son appétit insatiable le conduit à se tapir souvent là où transitent le plus de données. Supposé trouver un terroriste dans une botte de foin, il souffre pourtant souvent de myopie et d’obésité ,le rendant plus efficace pour chaparder des secrets des politiques et des industriels que pour cerner les méchants avant qu'ils ne passent a l'action.

### Qui mangent des données publiques 

Les différentes strates administratives de la force publique cultivent également des jardins florissants de données aux saveurs multiples: biométriques, fiscales, environnementales, urbaines, professionnelles ou encore liées à la santé.

D’apparence neutres et objectives, les bêtes algorithmiques publiques seraient la solution aux inégalités de traitement face au libre arbitre de certains fonctionnaires. Elles peuvent néanmoins métamorphoser des familles entières en insectes kafkaïens accrochés à la machine à écrire du film *Brazil* [^8]. Ce sont elles en effet qui déterminent désormais où votre enfant doit aller à l’école, si vous pouvez bénéficier d’aides sociales, à quel travail vous devriez postuler, si votre cycle menstruel est prêt pour procréer.

Les commerçants de la donnée personnelle proposent bien volontiers leur aide aux collectivités publiques pour numériser et cloner les plus belles plantes du jardin public, qu’il s’agisse de fleurs culturelles ou d’herbes médicinales. Tous comme les commerçants, la force publique est passée de l’observation à la prédiction, elle peut non seulement optimiser le ramassage des poubelles, mais aussi envoyer ses forces de police là où un délit à le plus de chances de se dérouler grâce à ses algochiens PredPol CompStat ou encore HunchLab [^9].

### Qui mangent de l’argent 

Thomas Peterffy est un financier qui s’est attaché à remplacer les courtiers et leurs opérations manuelles par des machines automatisées. En 1987, constatant que le nombre d’ordres passés par Peterffy était étonnamment élevé, les responsables des marchés envoient un inspecteur qui, là où il s’attendait à voir une salle des marchés remplie d’hommes blancs vociférants et suants, ne trouve qu’un ordinateur IBM relié au seul terminal officiel du Nasdaq [^10]. Ainsi, en 1987, les algorithmes sont lâchés sur les marchés financiers.

Aujourd’hui, l’algotrading est généralisé, les clignotements algorithmiques feutrés des réseaux informatiques ont remplacé les traders hystériques, mais ces bêtes numériques de la finance se font déjà dépasser par les algotraders à hautes fréquences. Ces derniers se déplacent à la vitesse de la lumière, bâtissent des chemins pour arriver à l’ordre d’achat et de vente plus vite que les autres [^11], engrangeant un bénéfice à chaque opération. Ils trouvent désormais abris dans les nombreux « dark pools » que les banques ont pu créer grâce à l’assouplissement paradoxal des réglementations. Dans ce confort lucratif qui connaît parfois tout de même des « Flash Crashs » [^12], la diversité des espèces algorithmiques s’accroît (Blast, Stealth, Sniffer, Iceberg, Shark, Sumo, … [^13]) de pair avec la complexité de leurs stratégies, rendant encore plus illisibles et incontrôlables les « marchés » pourtant supposés se réguler à grand coup de mains invisibles.

Tout ça impacte bien évidemment ce que l’on appelle « l’économie réelle » , c'est-à-dire la vie des gens. Par exemple, lorsque des pirates syriens compromettent le compte Twitter de la Maison Blanche et y postent un Tweet alarmiste qui est immédiatement lu par les robots algotraders, faisant ainsi plonger la bourse d’un seul élan de 136 Milliards de dollars en 3 minutes [^14].

Dans la jungle de la finance, une nouvelle bête algorithmique est apparue sous la forme d’un ver qui se duplique dans tous les ordinateurs accueillants et qui grossit au gré de son utilisation, dévorant au passage une quantité impressionnante d’électricité [^15]. On l’appelle la « blockchain » [^16] et elle s’est fait connaître via le « bitcoin », la première crypto-monnaie dématérialisée qui se passe d’organisme bancaire central attaché à un Etat. Le bitcoin pèse aujourd’hui 28 Milliards de dollars [^17].

Heureusement, des initiatives comme Ethereum [^18] ont permis aux vers de muter pour ne plus seulement enregistrer que des transactions mais aussi véhiculer des bases de données et des applications « intelligentes » (les « smart contracts »). Cela donne des projets comme la DAO [^19] (Decentralized Autonomous Organisation), un fonds d‘investissement décentralisé sans directoire où chacun prend part aux décisions en fonction de son capital. Ce fonds s’est vite retrouvé garni par de multiples investisseurs, pour un montant de 150 millions de dollars. Néanmoins, un malicieux plaisantin a réussi à en soustraire un tiers en exploitant une faille (une fonctionnalité dit-il) du code qui est irrémédiablement gravé dans le corps du ver DAO hébergé par Ethereum. Faut-il couper les anneaux du ver malade ou le tuer pour en créer un nouveau ? C’est la deuxième solution qui a été adoptée pour que les investisseurs récupèrent leur pécule, après moultes discussions « politiques », alors qu’ils partaient du principe libertarien selon lequel « le code fait loi ». Ce qui soulève des questions juridiques importantes, notamment pour définir les responsabilités dans un réseau distribué [^20] ou encore imaginer des formes de gouvernance de ce « code » qui supplante les lois des États dans certains domaines.

D’autres bêtes algorithmiques sont friandes d’argent et cherchent à remplacer les humains au travail, maximisant la productivité et les coûts, et contribuant ainsi à une plus grande concentration des capitaux. Les grandes entreprises l’ont bien compris et c'est ainsi que Foxconn annonce remplacer la quasi totalité de ses salariés par un millions de robots [^21] ou que le cabinet d’avocats BakerHostetler embauche l’intelligence artificielle ROSS pour étudier plus rapidement les complexes dossiers juridiques [^22]. La « mort du travail » est annoncée [^23] mais le régime économique et social supposé le remplacer peine à poindre à l’horizon.

### Qui mangent des cerveaux humains 

Dernières familles identifiées dans notre bestiaire algorithmique, celles dont le fond de commerce est de remplir le cerveau humain et celles qui se chargent au contraire de l’aspirer pour mieux le remplacer. Les intelligences artificielles doivent se nourrir de bonnes données pour pouvoir supplanter les humaines dans un plus grand nombre de processus. C’est notamment ce que fait Google avec le projet reCAPTCHA [^24],ces images illisibles qu’il faut décrypter et transcrire pour faire comprendre au serveur que nous ne sommes pas des robots mais bien des humains, on passe ainsi le test de Turing à l’envers [^25]. La grande innovation avec reCAPTCHA, c’est que le fruit de vos réponses nourrit directement les intelligences artificielles en suivant l’évolution des programmes de Google: décryptage de texte pour améliorer la numérisation des livres, identification des numéros de bâtiments pour affiner la cartographie et maintenant identification des images contenant des animaux ou panneaux de signalisation pour rendre le pilote automatique de la voiture moins myope. Cumulés, les résultats sont de plus en plus pertinents et représentent des millions d’heures de travail humain [^26].

Quant à l’algorithme qui contribue à nourrir notre cerveau, il est, comme son collègue collecteur de données personnelles, de plus en plus élaboré et subtil. On alimente son cerveau quotidiennement à l’aide d’un moteur de recherche qui va nous indiquer le lien le plus pertinent, l’information la plus juste, la vidéo la plus emblématique. Début 2017, dans 92,8 % des cas cela sera Google. Cela en fait un dictateur culturel dans une position hégémonique tout à fait originale (mais que font les autorités de la concurrence !?). Ne pas apparaître dès la première page de résultat, c’est comme ne pas exister. Pourtant, l’algorithme de recherche de Google est jalousement gardé en tant que secret industriel et peut juste se voir opposer un droit à l’oubli [^27].

Depuis la surréaliste expérience des chercheurs du laboratoire de Facebook [^28], réalisée en 2010 sur 61 millions d’utilisateurs, pendant les élections du congrès US, on sait que le contrôle des messages de mobilisation politique a une influence directe sur le vote des personnes cobayes à leur insu, ainsi que sur celui de leurs amis et amis d’amis. Depuis, les fausses nouvelles ont abondamment chassé les bonnes sur les réseaux sociaux, venant grossir le flot de la post-vérité. De quel bord politique sont les algorithmes qui président l’affichage du contenu sur nos « murs »? Si l’on y associe trop rapidement la problématique des discours d’incitation à la haine et de harcèlement sur ces plateformes, cela place les algorithmes et leurs dresseurs en position d’ordonnateur moral d’une bonne partie de la société.

On pourrait croire que pour atteindre plus rapidement le point de singularité technologique [^29], nos bêtes numériques tapies dans l’ombre s’ingénient à nous rendre serviles.

La gouvernementalité algorithmique [^30] serait ce nouveau mode de gouvernement des conduites, fruit de glissements dans notre rapport à l’autre, au groupe, au monde, au sens même des choses, qui ont, grâce ou à cause, du tournant numérique, des répercussions fondamentales sur la façon dont les normes se fabriquent et fabriquent l’obéissance [^31].

Quand un algorithme mange du cerveau humain, cela peut aussi provoquer la mort clinique de l’humain en question. Que dire des algorithmes qui pré-définissent des proies pour les drones tueurs, encore pilotés par des hommes et des femmes? Comment les algorithmes d’une voiture sans pilote choisissent le moindre mal / nombre de morts lorsqu’ils sont impliqués dans un accident qui va de toute façon en provoquer ? La cyberguerre plane sur nos prises réseaux, chaque pays affûte ses algorithmes pour être toujours plus sournoisement mortifère que l’ennemi.

## Comment peut-on savoir si un algorithme est méchant ou gentil?

Méchant algorithme que celui qui a transformé des caméras de surveillance en armada de botnets sanguinaires qui se précipitent en masse pour étrangler les serveurs? Gentil algorithme celui qui me rappelle l’anniversaire de mes amis? Pas si simple de formuler des critères considérant l’interdépendance entre l’algorithme, ses données et les intentions qui l’animent. Néanmoins, on pourrait espérer qu’un algorithme sympathique répondent à ceux-ci:

-   être « auditable » et donc constitué d’un code source ouvert et documenté
-   être « ouverts » et donc manger exclusivement des jeux de données ouverts (« open data »), complets et « moissonables » par d’autres, dont on pourrait idéalement discriminer l’accès pour rendre payant certains usages commerciaux
-   être « loyal et équitable », n’ayant pas la capacité de provoquer des discriminations ou des injustices (d’origine sociale[^32], de genre[^33], ...), de porter atteinte aux êtres humains [^34]
-   être « transparent [^35] » et capable de réaliser des comptes-rendus systématiques de ses opérations et évolutions s’il est doté de capacités d’apprentissage ou de prédiction, être en mesure de subir des contrôles citoyens
-   être « altérable » et pouvoir se prêter de façon légitime à des réclamations qui peuvent engendrer des modifications dans le fonctionnement de l’algorithme.

Dans cette quête de moralité algorithmique, il faut aussi évoquer les « portes », les API (pour *Application Programming Interface*), qui permettent à ces bêtes numériques d’aller chasser des données sur d’autres serveurs et services que les leurs, ou au contraire d’aller y poser des contenus, des appâts.. On peut considérer que ces APIs sont les pendants des brevets pour l’industrie, nouvelle forme de brevet logiciel anti open-source. Ces portes peuvent s’ouvrir et se fermer au gré des humeurs stratégiques de leur tenancier, ou se voir greffer un péage si le trafic d’un algo devient trop abondant, si cette monétisation devient opportune.

Dans la sphère publique et celle de la société civile, on peut imaginer que les critères antérieurement introduits d´ouverture, transparence, responsabilité, modificabilite soient respectés un jour. C’est plus compliqué à imaginer dans la sphère privée / lucrative, la donnée et les algos qui la consomme étant considérés comme « le pétrole de demain » [^36] …

Ainsi, un groupe de chercheurs américains et des « gros » du numérique ont tenté de formuler des « principes pour des algorithmes responsables » [^37] et se sont aussi réunis pour initier un partenariat sur l’éthique des Intelligences artificielles [^38], très bonne manière de dire aux élus et citoyens préoccupés qu’ils « anticipent et gèrent » très bien cette complexité et qu’il n’est surtout pas utile de légiférer.

Pourtant, l’enjeu n’est pas d’exiger la transparence du code des algorithmes, mais celle de leurs finalités. Tant qu’elles ne se réduisent pas à de la communication commerciale, passer par la loi reste un moyen de coercition indispensable à déployer [^39].Pour se réconforter, on notera le débat participatif en France, dans le cadre de la « Loi sur la république numérique » qui a conduit à instituer un devoir de transparence pour les algorithmes utilisés par la force publique [^40], ou encore l’initiative « TransAlgo » [^41] de l’INRIA qui vise à évaluer la responsabilité et la transparence des robots logiciels.

## Futurutopies algorithmiques souveraines

Alors comment passer d’une bête algorithmique que l’on subit à un animal de compagnie que l’on nourrit? Compostons quelques lombrics pour dessiner les ramifications biotechnologiques qui conduiront les hommes et la technologie à vivre dans une harmonie de silice. Ou comment peut on reprendre en main nos destinées, notre autonomie mentale, notre souveraineté technologique aujourd’hui algopropulsées au firmament du contrôle social?

Le code est un objet politique, tout comme ce monde « numérique » grouillant d’algobots qui s’incarnent bien dans nos réalités.

En tant qu’objet politique, on peut donc s’y attaquer avec des outils classiques: militantisme et lobbying didactique auprès des pouvoirs publics, tentatives pour infléchir et abonder le processus réglementaire, valorisation d’initiatives concourant à donner plus d’autonomie et de bonheur aux êtres humains. Il peut également être opportun de revendiquer une place plus importante pour la société civile dans les instances de régulation et de normalisation de l’Internet, l’adoption d’un standard pour une technologie du réseau [^42] étant par exemple l’équivalent d’un article dans la constitution d’un pays.

Au niveau individuel , il faut sans nul doute « dégoogliser » Internet [^43], c’est à dire, à l’instar de ce que propose l’association Framasoft, s’appuyer sur des hébergeurs de services autonomes, transparents, ouverts, neutres et solidaires (cf. initiative CHATONS [^44]), ou pourquoi pas s’autohéberger [^45] sur un mini-serveur peu gourmand. On peut aussi tenter de se camoufler en utilisant le chiffrement de bout en bout, ce qui n’est pas toujours simplement adaptable et adoptable (PGP et les mails…) selon les cas de figure, on peut alors avoir recours au brouillage en tentant de noyer la « vraie » donnée dans des données factices mais crédibles qu’un algorithme complice nous fournirait en abondance.

Du coté des pouvoirs publics, il y a du travail, la voie de la transparence éthique est ouverte, il ne reste plus qu’à les y pousser fermement. Il faut certes adopter aujourd’hui une coupe de cheveux et un maquillage [^46] bien étrange pour échapper aux systèmes de reconnaissance faciale [^47], au fichage biométrique, à la mise en relation des bases de données publiques, et les dérives numériques de l’état d’urgence désormais permanent nous invitent à ne pas mettre tous nos octets dans le même panier.

On peut aussi tout à fait prendre le parti de nourrir ces algoIAs avec des étrons, à l’instar de ces utilisateurs de Twitter qui ont réussi en moins d’une journée à transformer l’IA de Microsoft TAY en sauvageonne sexiste, raciste pro-Hitler [^48] … Imaginons plutôt élever de petits algoponeys qui viendrait déclamer, d’une ondulation de leur crinière arc-en-ciel sur fond de prairies verdoyantes de données, que « l’amitié c’est magique! ».

Mièvreries mises à part, il faut peut-être également intercaler un intermédiaire informatique, un « proxy » entre nous, nos données, et les acteurs publics et privés qui les accueillent. Cet intermédiaire pourrait héberger confortablement Eliza [^49], mon intelligence artificielle strictement personnelle qui se nourrit de mon activités et de mes préférences pour m’aider au mieux à partager mes données et contenus, en les anonymisant, en les donnant à des organismes publics dans une logique d’intérêt général, en les chiffrant ou les brouillant pour échanger avec mes amis qui n’arrivent pas à quitter les réseaux sociaux commerciaux. Distribuées dans la poche de chacun, les IA personnelles pourraient rentrer en symbiose, avec l’accord de leurs tuteurs, pour raconter à l’humanité des micro-fictions adaptées au contexte politique et culturel, afin de construire des réalités harmonieuses où cohabiteront en paix les algorithmes, les humains, la nature et le monde inorganique.

[^1]: Ce titre fais référence au livre de Cathy O’Neil. *Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy*. Crown, 6 septembre 2016.

[^2]: In this Isaac Asimov futuristic novel, the United States has converted to an "electronic democracy" where the computer Multivac selects a single person to answer a number of questions. Multivac will then use the answers and other data to determine what the results of an election would be, avoiding the need for an actual election to be held. https://en.wikipedia.org/wiki/Franchise_%28short_story%29

[^3]: https://fr.wikipedia.org/wiki/Bestiaire

[^4]: Cardon, Dominique: *A quoi rêvent les algorithmes: Nos vies à l’heure des big data*. Le Seuil, 2015.

[^5]: Morozov, Evgeny, et Pascale Haas. *Le mirage numérique: Pour une politique du Big Data*. Les Prairies Ordinaires, 2015.

[^6]: https://centenaire-shannon.cnrs.fr/chapter/la-theorie-de-information

[^7]: https://fr.wikipedia.org/wiki/PRISM_%28programme_de_surveillance%29

[^8]: Terry Gilliam: *Brazil* (1985) http://www.imdb.com/title/tt0088846/

[^9]: Cathy O’Neil. *Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy*. Crown, 6 septembre 2016

[^10]: Quelques jours après, il stipule à Peterfly que les ordres doivent être impérativement saisis sur le clavier du terminal et lui donne une semaine pour débrancher l’IBM. Dans ce laps de temps, celui-ci va embaucher des ingénieurs et construire un oeil-caméra qui lit l’écran, envoie les informations au cerveau IBM auquel on a greffé des mains électromécaniques qui peuvent saisir les ordres sur le clavier du terminal Nasdaq.

[^11]: Sniper In Mahwah: Anthropology, market structure & the nature of exchanges. https://sniperinmahwah.wordpress.com/

[^12]: Le Flash Crash du 6 mai 2010 analysé par Nanex: http://www.nanex.net/20100506/FlashCrashAnalysis_Intro.html et https://www.youtube.com/watch?v=E1xqSZy9_4I

[^13]: Laumonier Alexandre. 5/6. Zones Sensibles Editions, 2014. http://www.zones-sensibles.org/livres/6-5/

[^14]: https://www.washingtonpost.com/news/worldviews/wp/2013/04/23/syrian-hackersclaim-ap-hack-that-tipped-stock-market-by-136-billion-is-it-terrorism/

[^15]: Cette bête est tellement gourmande (une opération lui demande autant d’électricité qu’un foyer américain moyen pendant un jour et demi), qu’elle vit principalement en Chine et est maintenant très lente. https://motherboard.vice.com/read/bitcoin-is-unsustainable

[^16]: https://marmelab.com/blog/2016/04/28/blockchain-for-web-developers-thetheory.html

[^17]: Capitalisation et mouvements quotidiens des crypto-monnaies. https://coinmarketcap.com/

[^18]: https://www.ethereum.org/

[^19]: https://en.wikipedia.org/wiki/The_DAO_%28organization%29

[^20]: Ethereum: Freenet or Skynet? Primavera De Filippi, Berkman Center Fellow. https://cyber.harvard.edu/events/luncheon/2014/04/difilippi

[^21]: https://www.theverge.com/2016/12/30/14128870/foxconn-robots-automation-appleiphone-china-manufacturing

[^22]: https://www.washingtonpost.com/news/innovations/wp/2016/05/16/meet-ross-thenewly-hired-legal-robot/

[^23]: Bernard Stiegler. *La Société automatique. L'avenir du travail*. Fayard, 2015. http://www.philomag.com/les-livres/fiche-de-lecture/la-societe-automatique-1lavenir-du-travail-11454

[^24]: https://www.google.com/recaptcha/intro/index.html

[^25]: https://en.wikipedia.org/wiki/Turing_test

[^26]: https://www.bizjournals.com/boston/blog/techflash/2015/01/massachusettswomans-lawsuit-accuses-google-of.html

[^27]: https://www.google.com/webmasters/tools/legal-removal-request?complaint_type=rtbf

[^28]: A 61-million-person experiment in social influence and political mobilization. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3834737/

[^29]: https://fr.wikipedia.org/wiki/Singularit%C3%A9_technologique

[^30]: Antoinette Rouvroy et Thomas Berns. "Gouvernementalité algorithmique et perspectives d'émancipation: le disparate comme condition d'individuation par la relation?" Politique des algorithmes. Les métriques du web. *RESEAUX*, Vol.31, n.177, pp. 163-196 (2013). https://works.bepress.com/antoinette_rouvroy/47/

[^31]: ifapa.me est un collectif dédié à la recherche et la subversion des effets de la mathématisation et de la quantification de la vie quotidienne dans les sociétés nécrocapitalistes. http://www.ifapa.me/

[^32]: https://www.washingtonpost.com/opinions/big-data-may-be-reinforcing-racialbias-in-the-criminal-justice-system/2017/02/10/d63de518-ee3a-11e6-9973c5efb7ccfb0d_story.html?utm_term=.b7f5ab5df1f9

[^33]: https://www.genderit.org/feminist-talk/algorithmic-discrimination-andfeminist-politics

[^34]: https://fr.wikipedia.org/wiki/Trois_lois_de_la_robotique

[^35]: http://internetactu.blog.lemonde.fr/2017/01/21/peut-on-armer-la-transparencede-linformation/

[^36]: Documentaire « Le secret des 7 soeurs ». https://secretdes7soeurs.blogspot.fr/

[^37]: https://www.fatml.org/resources/principles-for-accountable-algorithms

[^38]: https://www.lemonde.fr/pixels/article/2016/09/28/intelligence-artificielleles-geants-du-web-lancent-un-partenariat-sur-l-ethique_5005123_4408996.html

[^39]: https://www.internetactu.net/2016/03/16/algorithmes-et-responsabilites/

[^40]: https://www.service-public.fr/particuliers/actualites/A11502

[^41]: https://www-direction.inria.fr/actualite/actualites-inria/transalgo

[^42]: The Internet Engineering Task Force (IETF®) https://www.ietf.org/

[^43]: https://degooglisons-internet.org/

[^44]: https://chatons.org/

[^45]: https://yunohost.org/

[^46]: https://cvdazzle.com/

[^47]: https://www.lemonde.fr/pixels/article/2016/10/19/inquietudes-autour-de-lareconnaissance-faciale-aux-etats-unis_5016364_4408996.html

[^48]: https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbotgets-a-crash-course-in-racism-from-twitter

[^49]: http://elizagen.org/

